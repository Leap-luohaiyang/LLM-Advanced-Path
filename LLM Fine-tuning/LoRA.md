# LoRA: Low-Rank Adaptation of Large Language Models
## 预备知识：
### 1、矩阵的秩：矩阵中线性无关的行或者列的最大数量
### 2、矩阵的秩低：意味着这个矩阵可以用更少的线性组合向量来表示，也就是说，它的内部结构相对简单，信息冗余较多
### 3、低秩矩阵可以被分解为两个“瘦”矩阵的乘积，这两个矩阵的维度远小于原始矩阵，这种分解体现了矩阵的低维表示

## 背景：
### 1、训练大模型的成本越来越高
### 2、大模型的权重矩阵当中，一部分参数是冗余的（10×10 的矩阵干的事情 2×10 的就能干）

## LoRA 的出发点：
### 1、核心思路是用两个低秩矩阵的乘积来代替传统的大规模权重矩阵的更新部分
### 2、在微调预训练模型时，LoRA不是去直接调整完整的权重矩阵，而是学习两个低秩矩阵的乘积，从而大幅减少要学习的参数数量
### 3、利用低秩矩阵可以有效逼近高维矩阵更新的性质，降低了存储和计算成本，同时保持调整能力

