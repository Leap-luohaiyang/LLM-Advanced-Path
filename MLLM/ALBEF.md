# Align before Fuse: Vision and Language Representation Learning with Momentum Distillation

### 多模态学习中较好的模型结构应当满足：
#### 1、图像和文本各自用不同的编码器先分别进行编码，然后再进行多模态融合
#### 2、图像编码器的网络规模应当大于文本编码器，这一点在 CLIP 中得到过验证，即增加视觉部分的网络大小，模型的精度会有所提升，增加文本部分的网络大小，模型的精度变化不大
#### 3、融合编码器不能太简单

### ALBEF 的模型架构
<img src="ALBEF.png"><br>
#### 左边为图像编码器，采用 ViT 结构，层数大于文本编码器
#### 右边为文本编码器，采用 Bert 结构
#### 每个编码器都有一个 [CLS] token 来提取图片和文本特征
#### 通过图像和文本特征的对比学习对齐图像和文本的特征表示
#### ALBEF 沿用了 MOCO 的思想，增加了动量模型（既有图像 Encoder 的参数，也有文本 Encoder 的参数），这使得动量模型能够通过队列给图像和文本分别提供一致且多的负例。训练模型生成的图像向量和动量模型的文本队列进行对比，训练模型生成的文本向量和动量模型的图像队列进行对比
#### 多模态融合部分的 cross-attention 中的 q 来自文本编码器，k 和 v 来自图像编码器
#### 使用 ITM（图文匹配）和 MLM 任务可以充分地进行多模态融合

### 何谓“动量蒸馏”？
