1. Q：详细说一下 Decoder 的因果注意力 Q、K、V 分别来自哪  
A：因果关系改进主要是防止模型访问序列中的未来信息。
在 Transformer 的自回归任务中，每个词的预测只能依赖于之前的词。与标准自注意力允许访问整个输入序列不同，
因果自注意力会遮蔽序列中当前位置之后的输入，限制模型在计算注意力得分时，只能访问当前位置及之前的 token，因此也被称为遮蔽注意力（masked attention）
Decoder 的因果注意力中，O、K、V 均来自输出序列  
<img src="Image/casual_attention.jpg">  
</br>  
2. Q：Attention 为什么要做 scaled？不做会怎么样？为什么要除以 $\sqrt{d_k}$？  
A：假设向量 $q$ 和 $k$ 的各个分量是互相独立的随机变量，均值为 0，方差为 1，那么点积 $q \cdot k$ 的均值为 0，方差为 $d_k$。所以当 $d_k$ 变大时意味着 $q \cdot k$ 的方差变大，从而导致对 softmax 函数求导时出现梯度消失的问题。除以 $\sqrt{d_k}$ 实际上是将 $q \cdot k$ 的方差控制为 1，从而有效避免梯度消失的问题  
更具体的回答见：https://www.zhihu.com/question/339723385/answer/811341890  
softmax 求导的公式推导：https://mingchao.wang/CPxjCCZa/
</br>
3. 